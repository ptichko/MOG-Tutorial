---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "Figures/",
  out.width = "100%"
)
```
# Mixture of Gaussian Models for Infants' Statistical Learning of Speech

<!-- badges: start -->
<!-- badges: end -->

A tutorial using the R library Mixtools to model infants' statistical learning of speech-sound categories.

## The Model

Research suggests that infants learn speech-sound categories from distributional features of speech cues, such as the voice onset time (VOT) of voiced and voiceless speech sounds. One popular computational model of this learning process is the Mixture of Gaussian (MOG) model. In this tutorial, a MOG model, whose parameters are estimated using the Expectation-Maximization (EM) algorithm, is used to simulate infants' learning of speech sounds. First, a model is introduced to estimate parameters of VOT distrbutions for voiced and voiceless speech sounds using Expectation Maximization (EM) with a known number of categories. Then, a hypothesis-testing approach is introduced to determine the number of categories. The model is loosely based off of McMurray et al. (2009) (https://doi.org/10.1111/j.1467-7687.2009.00822.x) and de Boer & Kuhl (2003) (https://doi.org/10.1121/1.1613311). Code for running and visualizing the MOGs was adapted from Cosma Shalizi's lecture on MOGs: https://www.stat.cmu.edu/~cshalizi/402/lectures/20-mixture-examples/lecture-20.pdf

## Running the Model
The main R script to run the model can be found in MOG-Tutorial.R. First, a dataframe of VOTs is created natively in R, with parameters corresponding to voice and voiceless speech sounds, based on VOT data cited in McMurray et al. (2009). A histogram of the simulated data clearly shows the two speech-sound "categories" (i.e., a bi-modal distribution of the VOT data), each representing the VOTs for voiced and voiceless speech sounds:

```{r MOG_2_Run, include = FALSE}

#Mixtools is an R library for MOG
# install.packages("mixtools") #Mixtools has built in MOG models with the EM algorithm
library(mixtools)

####Simulate VOT language input####
#Simulated input parameters taken from McMurray et al. (2009), table 1, row 1
#Bimodel distrbution of VOTs reflecting voiced and voiceless speech sounds
#voiced parameters: mean = 0, sd = 5, lambda = 0.5
mu.vd <- 0
sd.vd <- 5
lam.vd <- 0.5

#voiceless: mean = 50, sd = 15, lambda = 0.5
mu.vl <- 50
sd.vl <- 15
lam.vl <- 0.5

#number of samples
n = 1000

#set seed
set.seed(1)

#Sample from normal distribution and create voiced/voiceless factor
VOTs <- c(rnorm(n, mean = mu.vd, sd = sd.vd),rnorm(n, mean = mu.vl, sd = sd.vl))
VOT.type <- factor(rep(c("voiced", "voiceless"), times = c(n,n)))

#Create data frame
VOT.data <- data.frame(VOT.type, VOTs)

head(VOT.data)
dim(VOT.data)

#Plot density by voiced/voiceless
# lattice::histogram(~ VOTs | VOT.type, data = VOT.data, type = "density", breaks = 50)

#Plot density of all VOT data
hist(VOT.data$VOTs, breaks = 75, freq = FALSE, col="grey",border="grey", xlab = "VOT (Milliseconds)", main = "Voice Onset Times (VOTs) of Voiced and Voiceless")
lines(density(VOT.data$VOTs), lwd = 2)

#### Initialize MOG model with a prioi # of Guassians/cateogires (e.g. K) ####

#We know a priori that the distribution of VOTs is bimodal. Let's try 2 components.
k <- 2 # num of Gaussians/categories

#Run model
# k = number of components/Gaussians
# maxit = max number of iterations for converging the model
# epsilon = run until the log-likelihood changes by less than epsilon. Usually set to 10^-8
mixmdl <- normalmixEM(VOT.data$VOTs, k = k, maxit = 100, epsilon = 0.01)

```

```{r VOT_plot, echo = FALSE}
#Plot density of all VOT data
hist(VOT.data$VOTs, breaks = 75, freq = FALSE, col="grey",border="grey", xlab = "VOT (Milliseconds)", main = "VOT of Voiced and Voiceless")
lines(density(VOT.data$VOTs), lwd = 2)

```


```{r VOT_parameters}

# voiced parameters: mean = 0, sd = 5, lambda = 0.5
mu.vd <- 0
sd.vd <- 5
lam.vd <- 0.5

# voiceless: mean = 50, sd = 15, lambda = 0.5
mu.vl <- 50
sd.vl <- 15
lam.vl <- 0.5

```


Then, a MOG model is introduced with k = 2 Gaussians to model k = 2 speech-sound categories. The model correctly estimates the parameters of each Gaussian used to generate the simulated data. In the figure below, the dashed lines reflect the estimated Gaussians from the MOG model. Here, we find that the estimated Gaussians capture the distribution of VOTs for each speech-sound category quite nicely: 


```{r MOG_2_Plot, echo=FALSE, message = FALSE}
# Plot density of VOT data with model fit
#Function that adds the density of a given component of the MOG; scaled by lambda for comparable visualization
plot.normal.components <- function(mixture,component.number,...) {
  curve(mixture$lambda[component.number] *
          dnorm(x,mean=mixture$mu[component.number],
                sd=mixture$sigma[component.number]), add=TRUE, ...)
}

hist(VOT.data$VOTs,breaks=75,col="grey",border="grey",freq=FALSE,
     xlab="VOT (Milliseconds)",main="Estimated Gaussians (k = 2) for Voiced and Voiceless")
#lines(density(VOT.data$VOTs),lty=2)
#sapply(1:k,plot.normal.components,mixture=mixmdl, lty = 2)
plot.normal.components(mixture=mixmdl, component.number = 1, lty = 2)
plot.normal.components(mixture=mixmdl, component.number = 2, lty = 3)
```


```{r MOG_2_Output}
# Hey! The fitted parameters match the parameters of the simulated data quite well.
mixmdl$mu
mixmdl$sigma
mixmdl$lambda
```


However, this is approach is not developmentally plausible, since we know a priori what the correct value of k should be (i.e., 2 for two speech-sound categories), while infants who are learning a spoken language do not. Next, a hypothesis-testing approach is introduced, in which the MOG model learns the "correct" number of k through bootstrapping and hypothesis-testing. The p-values correspond to tests between a model feature k categories and a model featuring k+1 categories. Here, we see that the first p-value, reflecting a model comparision between k = 1 vs. k = 2, is significant. However, the p-value for a model comparision k = 2 vs. k = 3 is not. Thus, the model correctly converged to two categories. 

```{r MOG_Hypothesis_Run, include = FALSE}

####Initialize MOG model to have it learn K through bootstrapping and hypothesis testing####
mixmdl.2 <- boot.comp(VOT.data$VOTs,max.comp=15,mix.type="normalmix",
                      maxit=100,epsilon=1e-2)

```


```{r MOG_Hypothesis_Output}
# Hypothesis tests for k = 1 vs. k = 2 and k = 2 vs. k = 3.
mixmdl.2$p.values

```


## Developmental Caveats
McMurray et al. (2009) argue directly against using MOGs with the EM algorithm to model infant statistical learning for two reasons: first, the EM algorithm uses a large batch of input, but infants learn iteratively (i.e., they do not get "batch" data). Second, the EM algorithm doesn't learn the number of Gaussians/categories (the number is specified a priori #), only the parameters of the Gaussians are estimated. While here, we've introduced a hypothesis-testing procedure to determine the "correct" number for k, in their paper, McMurray et al. (2009) discard the EM alogrithm and use maximum likelihood estimation with a competition algorithm.
